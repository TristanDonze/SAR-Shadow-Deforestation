{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ship Detection - Tristan Donzé & Liam Loughman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract \n",
    "\n",
    "This notebook contains both the code implementation and report for a ship detection project inspired by the research paper \"Optimal Target Detection Using One Channel SAR Complex Imagery\" by Armand Lopès, Jérôme Bruniquel, Franck Sery, Jean-Claude Souyris, and Frédéric Adragna. The project focuses on detecting ships in Sentinel-1 SAR imagery using statistical methods. By implementing a combination of Statistical Whitening Filter (SWF) and Likelihood Ratio Test (LRT), followed by radiometric thresholding and morphological operations, we developed a robust non-learning approach for maritime vessel detection. Our methodology was applied to images from the Strait of Malacca, demonstrating effective ship detection capabilities in various maritime conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "**Optical Sensors**:\n",
    "\n",
    "- Advantages: High spatial resolution; rich spectral information useful for identifying material properties; effective in clear weather and daylight.\n",
    "\n",
    "- Drawbacks: Limited usability in cloudy or night conditions; atmospheric distortion can affect data quality.\n",
    "\n",
    "**SAR Sensors**:\n",
    "\n",
    "- Advantages: Operates day and night; penetrates clouds and adverse weather conditions; provides structural and geometric information.\n",
    "\n",
    "- Drawbacks: Lower spatial resolution compared to optical sensors; speckle noise complicates image processing; requires advanced preprocessing for analysis.\n",
    "\n",
    "**LiDAR**:\n",
    "\n",
    "- Advantages: Provides precise 3D topographic information; effective for height and structure analysis.\n",
    "\n",
    "- Drawbacks: Expensive; less effective in wide-area maritime applications compared to SAR; weather dependence due to laser attenuation.\n",
    "\n",
    "**SAR Altimeter**:\n",
    "\n",
    "- Advantages: Measures precise sea surface heights; useful for detecting wakes and identifying the maritime environment.\n",
    "\n",
    "- Drawbacks: Limited spatial resolution; not specifically designed for detecting small targets like ships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "**Sensor Selection:**\n",
    "SAR sensors were exclusively used in this project, reflecting their robustness for ship detection regardless of weather and illumination conditions. No multimodal approach was used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "**Image Source:**\n",
    "\n",
    "Open-access HR SAR images from Sentinel-1 from the straight of Malacca were used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Non-Learning Approach Used:\n",
    "\n",
    "The project relied on statistical and radiometric methods for ship detection:\n",
    "\n",
    "1. Spatial Whitening Filter (SWF): Preprocessing to normalize background clutter by computing mean and standard deviation of the background regions.\n",
    "\n",
    "2. Likelihood Ratio Test (LRT): A statistical approach to distinguish between background and target regions by comparing radiometric characteristics.\n",
    "\n",
    "3. Radiometric Thresholding: Setting thresholds based on desired False Alarm Rates (FAR) to isolate ships from the background clutter.\n",
    "\n",
    "4. Morphological Operations: Applied for postprocessing, such as removing small objects and filling holes, to refine ship detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing: \n",
    "\n",
    "### Download the Image\n",
    "\n",
    "The original data was downloaded from the [Copernicus Browser](https://browser.dataspace.copernicus.eu). You can find it by searching for ```S1A_IW_SLC__1SDV_20241228T113437_20241228T113505_057192_070894_70FE``` in the search bar.\n",
    "\n",
    "<img src=\"image.png\" alt=\"Straight of Malacca\" width=\"700\"/>\n",
    "\n",
    "### Preprocessing\n",
    "We then used the SNAP software (version 9.0.0) to preprocess the image. After opening the zip file, we applied the following steps: \n",
    "- **Radiometric Calibration**: This step converts pixel values into radar reflectivity values (sigma naught, σ°). To reproduce this in SNAP, use the tool ```Radar > Radiometric > Calibration``` and check only the \"Output sigma0 band\" option.\n",
    "\n",
    "- **Creating an Intensity Image**: The real (i) and imaginary (q) components are combined to obtain the signal amplitude, which is less sensitive to speckle than individual i and q values. To reproduce this in SNAP, use the tool ```Raster > Band Maths```, and input the expression `sqrt(Sigma0_IW1_VV)` followed by `sqrt(Sigma0_IW2_VV)`. (We used the VV channel, which provides better contrast for detecting ships because it is more sensitive to vertical reflective structures and metallic surfaces.)\n",
    "\n",
    "- **Exporting the Images**: Finally, we exported the images in `.tif` format. To reproduce this in SNAP, click on ```File > Export > Other > View as Image > Image Region: Full Scene, File Format: Tiff - Tagged Image File Format```.\n",
    "\n",
    "The original image of the Strait of Malacca was divided into three sub-images arranged left to right. Since the right sub-image mostly consists of land, we will focus only on the left and center sub-images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "from scipy.stats import norm\n",
    "from skimage import morphology\n",
    "from skimage.measure import label, regionprops\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.patches import Rectangle\n",
    "from shapely.geometry import box, MultiPolygon\n",
    "from shapely.ops import unary_union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Preprocessed Images\n",
    "\n",
    "We begin by loading the preprocessed .tif images using rasterio. This allows us to access the image data and its metadata for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [\n",
    "    'S1A_IW1_SLC_Cal_vv_intensity_VV_IW1.tif',\n",
    "    'S1A_IW1_SLC_Cal_vv_intensity_VV_IW2.tif',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = {}\n",
    "\n",
    "for path in file_paths:\n",
    "    with rasterio.open(path) as src:\n",
    "        image = src.read(1)\n",
    "        image_data[path] = {\n",
    "            'data': image,\n",
    "            'profile': src.profile\n",
    "        }\n",
    "        print(f\"Loaded {path} with shape {image.shape} and dtype {image.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Preprocessed Images\n",
    "\n",
    "To understand the data better, we visualize the preprocessed images. This helps in identifying features and potential areas of interest, such as ships.\n",
    "\n",
    "The below code will display grayscale images of the preprocessed data with amplitude values in decibels (dB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image(image, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.colorbar(label='Amplitude (dB)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for path, data in image_data.items():\n",
    "    visualize_image(data['data'], f\"Preprocessed Image\\n{path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Whitening Filter (SWF)\n",
    "\n",
    "#### Selecting Background Regions\n",
    "\n",
    "To enhance the detection of ships, we first identify background regions in the image. These regions are typically areas with minimal activity, such as open water, and are used to estimate background statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mask(mask, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    mask_uint8 = mask.astype(np.uint8)\n",
    "    plt.imshow(mask_uint8, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.colorbar(label='Mask Value')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def select_background_region(image, percentile=5):\n",
    "    thresh = np.percentile(image, percentile)\n",
    "    background_mask = image <= thresh\n",
    "    return background_mask\n",
    "\n",
    "for path, data in image_data.items():\n",
    "    image = data['data']\n",
    "    background_mask = select_background_region(image, percentile=15)\n",
    "    data['background_mask'] = background_mask\n",
    "    \n",
    "    visualize_mask(background_mask, f\"Background Mask - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The background masks highlight areas considered as background based on the specified percentile threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Statistical Whitening Filter (SWF)\n",
    "\n",
    "Statistical Whitening Filter normalizes the image by subtracting the mean and dividing by the standard deviation of the background. This enhances the contrast between foreground objects (ships) and the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_swf(image, mean_bg, std_bg):\n",
    "    whitened = (image - mean_bg) / std_bg\n",
    "    return whitened\n",
    "\n",
    "def estimate_background_stats(image, background_mask):\n",
    "    background_pixels = image[background_mask]\n",
    "    mean_bg = np.mean(background_pixels)\n",
    "    std_bg = np.std(background_pixels)\n",
    "    return mean_bg, std_bg\n",
    "\n",
    "for path, data in image_data.items():\n",
    "    image = data['data']\n",
    "    \n",
    "    mean_bg, std_bg = estimate_background_stats(image, data['background_mask'])\n",
    "\n",
    "    data['mean_bg'] = mean_bg\n",
    "    data['std_bg'] = std_bg \n",
    "    \n",
    "    whitened_image = apply_swf(image, mean_bg, std_bg)\n",
    "    data['whitened'] = whitened_image\n",
    "\n",
    "    visualize_image(whitened_image, f\"Whitened Image - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whitened images display normalized values, making it easier to identify anomalies such as ships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood Ratio Test (LRT)\n",
    "\n",
    "#### Computing LRT Statistics\n",
    "\n",
    "The Likelihood Ratio Test (LRT) is used to distinguish between background and foreground (ship) pixels based on their statistical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lrt_statistic(whitened_image, mean_bg, std_bg, mean_fg, std_fg):\n",
    "    log_term = np.log(std_bg / std_fg)\n",
    "    bg_term = ((whitened_image - mean_bg) ** 2) / (2 * std_bg ** 2)\n",
    "    fg_term = ((whitened_image - mean_fg) ** 2) / (2 * std_fg ** 2)\n",
    "    lrt_stat = log_term + bg_term - fg_term\n",
    "    return lrt_stat\n",
    "\n",
    "for path, data in image_data.items():\n",
    "    foreground_mask = ~data['background_mask']\n",
    "    image = data['data']\n",
    "    whitened_image = data['whitened']\n",
    "    data['mean_fg'] = np.mean(image[foreground_mask])\n",
    "    data['std_fg'] = np.std(image[foreground_mask])\n",
    "    print(data['mean_bg'], data['std_bg'], data['mean_fg'], data['std_fg'])\n",
    "    lrt_stat = compute_lrt_statistic(whitened_image, data['mean_bg'], data['std_bg'], data['mean_fg'], data['std_fg'])\n",
    "    data['lrt_stat'] = lrt_stat\n",
    "    \n",
    "    visualize_image(lrt_stat, f\"LRT Statistic - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LRT statistic image emphasizes regions where the likelihood of being a ship is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing LRT Statistics\n",
    "\n",
    "Normalization scales the LRT statistics to a standard range, facilitating thresholding and subsequent detection steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lrt(lrt_image):\n",
    "    mean_lrt = np.mean(lrt_image)\n",
    "    std_lrt = np.std(lrt_image)\n",
    "    normalized_lrt = (lrt_image - mean_lrt) / std_lrt\n",
    "    return normalized_lrt, mean_lrt, std_lrt\n",
    "\n",
    "for path, data in image_data.items():\n",
    "    lrt_image = data['lrt_stat']  # Replace with your computed LRT statistics\n",
    "    normalized_lrt, mean_lrt, std_lrt = normalize_lrt(lrt_image)\n",
    "    data['normalized_lrt'] = normalized_lrt\n",
    "    \n",
    "    print(f\"{path}: Mean (before) = {np.mean(lrt_image):.4f}, Std (before) = {np.std(lrt_image):.4f}\")\n",
    "    print(f\"{path}: Mean (normalized) = {np.mean(normalized_lrt):.4f}, Std (normalized) = {np.std(normalized_lrt):.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(normalized_lrt, cmap='gray', norm=Normalize(vmin=0, vmax=1))\n",
    "    plt.title(f\"Normalized LRT - {path}\")\n",
    "    plt.colorbar(label='Normalized LRT Value')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized LRT images have a mean of 0 and a standard deviation of 1, making it easier to apply consistent thresholds across images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radiometric Thresholding\n",
    "\n",
    "#### Computing Thresholds Based on False Alarm Rate (FAR)\n",
    "\n",
    "Thresholding helps in distinguishing between true detections (ships) and false alarms. We compute thresholds corresponding to specific FARs using the standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_threshold(FAR):\n",
    "    threshold = norm.ppf(1 - FAR)\n",
    "    return threshold\n",
    "\n",
    "FAR = [3e-1, 1e-1]\n",
    "\n",
    "thresholds = {}\n",
    "\n",
    "for i, path in enumerate(file_paths):    \n",
    "    threshold = compute_threshold(FAR[i])\n",
    "    print(f\"Threshold for {path} at FAR {FAR[i]}: {threshold:.4f}\")\n",
    "    thresholds[path] = threshold\n",
    "\n",
    "for path, data in image_data.items():\n",
    "    normalized_lrt = data['normalized_lrt']\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(normalized_lrt.flatten(), bins=100, color='blue', alpha=0.7)\n",
    "    plt.title(f\"LRT Histogram - {path}\")\n",
    "    plt.xlabel(\"LRT Statistic\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.axvline(thresholds[path], color='red', linestyle='dashed', linewidth=2, label='Threshold')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms display the distribution of normalized LRT values with the computed thresholds indicated by red dashed lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Thresholds to Detect Ships\n",
    "\n",
    "Using the computed thresholds, we classify pixels as detections (potential ships) or non-detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_threshold(lrt_stat, threshold):\n",
    "    detections = lrt_stat > threshold\n",
    "    return detections\n",
    "\n",
    "for path, data in image_data.items():\n",
    "    normalized_lrt = data['normalized_lrt']\n",
    "    detections = apply_threshold(normalized_lrt, thresholds[path])\n",
    "    data['detections'] = detections\n",
    "    \n",
    "    visualize_image(detections, f\"Detections (FAR={FAR}) - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detection masks highlight areas where the LRT statistic exceeds the threshold, indicating potential ship locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Detections\n",
    "\n",
    "#### Removing Small Artifacts\n",
    "\n",
    "To improve detection accuracy, we remove small objects and fill small holes within the detection masks. This helps in eliminating noise and insignificant detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_detections(detections, min_size=20, area_threshold=20):\n",
    "    detections_clean = morphology.remove_small_holes(detections, area_threshold=area_threshold)\n",
    "    detections_clean = morphology.remove_small_objects(detections_clean, min_size=min_size)\n",
    "    return detections_clean\n",
    "\n",
    "for path, data in image_data.items():\n",
    "    detections = data['detections']\n",
    "    detections[data['background_mask']] = 0\n",
    "    detections_clean = clean_detections(detections, min_size=100, area_threshold=1000)\n",
    "    data['detections_clean'] = detections_clean\n",
    "    \n",
    "    visualize_image(detections_clean, f\"Cleaned Detections - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaned detection masks are more refined, showing fewer false positives and clearer ship outlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Counting of Detected Ships\n",
    "\n",
    "### Identifying Connected Components\n",
    "\n",
    "We count the number of detected ships by identifying connected regions in the cleaned detection masks. Each connected region is assumed to correspond to an individual ship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ships(detections_clean):\n",
    "    labeled_detections = label(detections_clean, connectivity=2)\n",
    "    regions = regionprops(labeled_detections)\n",
    "    num_ships = len(regions)\n",
    "    return num_ships, regions\n",
    "\n",
    "for path, data in image_data.items():\n",
    "    detections_clean = data['detections_clean']\n",
    "    num_ships, regions = count_ships(detections_clean)\n",
    "    data['num_ships'] = num_ships\n",
    "    print(f\"Number of detected ships in {path}: {num_ships}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Detections with Bounding Boxes\n",
    "\n",
    "#### Drawing Bounding Boxes Around Ships\n",
    "\n",
    "To visualize the detected ships, we draw bounding boxes around each detected region on the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bounding_boxes(original_image, regions, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(original_image, cmap='gray')\n",
    "    plt.title(title)\n",
    "    plt.colorbar(label='Amplitude (dB)')\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    for region in regions:\n",
    "        minr, minc, maxr, maxc = region.bbox\n",
    "        rect = Rectangle((minc, minr), maxc - minc, maxr - minr,\n",
    "                         linewidth=1, edgecolor='yellow', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for path, data in image_data.items():\n",
    "    original_image = data['data']\n",
    "    detections_clean = data['detections_clean']\n",
    "    num_ships = data['num_ships']\n",
    "    regions = label(detections_clean, connectivity=2)\n",
    "    props = regionprops(regions)\n",
    "    title = f\"Detected Ships ({num_ships}) - {path}\"\n",
    "    draw_bounding_boxes(original_image, props, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refining Detections\n",
    "\n",
    "#### Merging Bounding Boxes Based on Proximity\n",
    "\n",
    "To handle cases where a single ship might be detected as multiple regions, we merge bounding boxes that are in close proximity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bounding_boxes(detections_clean, proximity_threshold=20):\n",
    "    \n",
    "    labeled = label(detections_clean)\n",
    "    regions = regionprops(labeled)\n",
    "    \n",
    "    boxes = [box(region.bbox[1], region.bbox[0], region.bbox[3], region.bbox[2]) for region in regions]\n",
    "    \n",
    "    if not boxes:\n",
    "        return detections_clean.copy()\n",
    "    \n",
    "    buffered_boxes = [b.buffer(proximity_threshold) for b in boxes]\n",
    "    \n",
    "    merged = unary_union(buffered_boxes)\n",
    "    \n",
    "    refined_detections = np.zeros_like(detections_clean, dtype=bool)\n",
    "    \n",
    "    if isinstance(merged, MultiPolygon):\n",
    "        geometries = merged.geoms\n",
    "    elif merged.geom_type == 'Polygon':\n",
    "        geometries = [merged]\n",
    "    else:\n",
    "        geometries = [] \n",
    "\n",
    "    for geom in geometries:\n",
    "        minx, miny, maxx, maxy = geom.bounds\n",
    "        min_row, min_col = int(np.floor(miny)), int(np.floor(minx))\n",
    "        max_row, max_col = int(np.ceil(maxy)), int(np.ceil(maxx))\n",
    "        \n",
    "        min_row = max(min_row, 0)\n",
    "        min_col = max(min_col, 0)\n",
    "        max_row = min(max_row, detections_clean.shape[0])\n",
    "        max_col = min(max_col, detections_clean.shape[1])\n",
    "        \n",
    "        refined_detections[min_row:max_row, min_col:max_col] = 1\n",
    "\n",
    "    return refined_detections\n",
    "\n",
    "for path, data in image_data.items():\n",
    "    detections_clean = data['detections_clean']\n",
    "    refined_detections = merge_bounding_boxes(detections_clean, proximity_threshold=30) \n",
    "    data['refined_detections'] = refined_detections\n",
    "\n",
    "    num_ships, regions = count_ships(refined_detections)\n",
    "    data['num_ships_refined'] = num_ships\n",
    "    print(f\"Number of detected ships after first refinement in {path}: {num_ships}\")\n",
    "    \n",
    "    labeled_refined = label(refined_detections, connectivity=2)\n",
    "    props_refined = regionprops(labeled_refined)\n",
    "    draw_bounding_boxes(data['data'], props_refined, f\"First Refined Detected Ships ({num_ships}) - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refined detections reduce duplicate counts and ensure each ship is represented by a single bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering Detections by Size\n",
    "\n",
    "To further enhance detection accuracy, we filter out detections that are either too small or too large to be ships based on area constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_detections_by_size(detections_clean, max_area=5000):\n",
    "    \n",
    "    labeled = label(detections_clean, connectivity=2)\n",
    "    regions = regionprops(labeled)\n",
    "    \n",
    "    filtered_detections = np.zeros_like(detections_clean, dtype=bool)\n",
    "    \n",
    "    for region in regions:\n",
    "        area = region.area\n",
    "        min_row, min_col, max_row, max_col = region.bbox\n",
    "        \n",
    "        if area <= max_area:\n",
    "            filtered_detections[min_row:max_row, min_col:max_col] = 1\n",
    "        else:\n",
    "            print(f\"Removed detection at [(Row: {min_row}, Col: {min_col}) to (Row: {max_row}, Col: {max_col})] \"\n",
    "                  f\"with Area: {area}\")\n",
    "    \n",
    "    return filtered_detections\n",
    "\n",
    "for path, data in image_data.items():\n",
    "    refined_detections = data['refined_detections']\n",
    "    \n",
    "    filtered_detections = filter_detections_by_size(refined_detections, max_area=16000)\n",
    "    data['filtered_detections'] = filtered_detections\n",
    "    \n",
    "    num_ships_filtered, regions_filtered = count_ships(filtered_detections)\n",
    "    data['num_ships_filtered'] = num_ships_filtered\n",
    "    print(f\"Number of detected ships after size filtering in {path}: {num_ships_filtered}\")\n",
    "    \n",
    "    labeled_filtered = label(filtered_detections, connectivity=2)\n",
    "    props_filtered = regionprops(labeled_filtered)\n",
    "    draw_bounding_boxes(data['data'], props_filtered, f\"Filtered Detected Ships ({num_ships_filtered}) - {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size filtering removes unlikely ship detections based on predefined area thresholds, resulting in a more accurate ship count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Some ships were not correctly detected, and certain land areas (primarily islands) were mistakenly identified as ships. Additionally, we only used two images, so the approach was not fully generalized to all possible cases. Nevertheless, we are satisfied with the results obtained, as the majority of the ships were successfully detected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
